<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Iara - Prof. Alberto Ferreira de Souza | Estudo e Aplicação de Técnicas de Machine Learning</title>
    <meta name="generator" content="VuePress 1.9.7">
    
    <meta name="description" content="Todos os relatórios e afins sobre machine learning, deep learning e Visão computacional estarão aqui.">
    
    <link rel="preload" href="/machineLearning/assets/css/0.styles.f80be0c7.css" as="style"><link rel="preload" href="/machineLearning/assets/js/app.85d7bd56.js" as="script"><link rel="preload" href="/machineLearning/assets/js/2.4f8302c5.js" as="script"><link rel="preload" href="/machineLearning/assets/js/25.40c583f5.js" as="script"><link rel="prefetch" href="/machineLearning/assets/js/10.29b60263.js"><link rel="prefetch" href="/machineLearning/assets/js/11.c63c2c0c.js"><link rel="prefetch" href="/machineLearning/assets/js/12.78249c11.js"><link rel="prefetch" href="/machineLearning/assets/js/13.fcdeb0ea.js"><link rel="prefetch" href="/machineLearning/assets/js/14.ced299f6.js"><link rel="prefetch" href="/machineLearning/assets/js/15.df6f2744.js"><link rel="prefetch" href="/machineLearning/assets/js/16.58f755ed.js"><link rel="prefetch" href="/machineLearning/assets/js/17.ce969e99.js"><link rel="prefetch" href="/machineLearning/assets/js/18.47c54fbb.js"><link rel="prefetch" href="/machineLearning/assets/js/19.73db5433.js"><link rel="prefetch" href="/machineLearning/assets/js/20.4168cf0b.js"><link rel="prefetch" href="/machineLearning/assets/js/21.320089d5.js"><link rel="prefetch" href="/machineLearning/assets/js/22.1740eda5.js"><link rel="prefetch" href="/machineLearning/assets/js/23.269b9116.js"><link rel="prefetch" href="/machineLearning/assets/js/24.850bbadc.js"><link rel="prefetch" href="/machineLearning/assets/js/26.d2dc0497.js"><link rel="prefetch" href="/machineLearning/assets/js/27.c3a0af4f.js"><link rel="prefetch" href="/machineLearning/assets/js/28.a01a0b69.js"><link rel="prefetch" href="/machineLearning/assets/js/29.c2a025ff.js"><link rel="prefetch" href="/machineLearning/assets/js/3.7ee3d1e9.js"><link rel="prefetch" href="/machineLearning/assets/js/30.82d06992.js"><link rel="prefetch" href="/machineLearning/assets/js/31.855a9550.js"><link rel="prefetch" href="/machineLearning/assets/js/4.c3fb2158.js"><link rel="prefetch" href="/machineLearning/assets/js/5.02305c60.js"><link rel="prefetch" href="/machineLearning/assets/js/6.4a48f0f1.js"><link rel="prefetch" href="/machineLearning/assets/js/7.7322ff41.js"><link rel="prefetch" href="/machineLearning/assets/js/8.198f804d.js"><link rel="prefetch" href="/machineLearning/assets/js/9.0acdc198.js">
    <link rel="stylesheet" href="/machineLearning/assets/css/0.styles.f80be0c7.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/machineLearning/" class="home-link router-link-active"><!----> <span class="site-name">Estudo e Aplicação de Técnicas de Machine Learning</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/machineLearning/artigos/sobre.html" class="nav-link">
  Artigos
</a></div><div class="nav-item"><a href="/machineLearning/glossario/funcoes/" class="nav-link">
  Glossário
</a></div><div class="nav-item"><a href="/machineLearning/relatorios/relatorio.html" class="nav-link">
  Relatorios
</a></div><div class="nav-item"><a href="/machineLearning/veiculos/intro.html" class="nav-link">
  Veículos Autônomos
</a></div><div class="nav-item"><a href="/machineLearning/tcc/index.html" class="nav-link">
  TCC
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/machineLearning/artigos/sobre.html" class="nav-link">
  Artigos
</a></div><div class="nav-item"><a href="/machineLearning/glossario/funcoes/" class="nav-link">
  Glossário
</a></div><div class="nav-item"><a href="/machineLearning/relatorios/relatorio.html" class="nav-link">
  Relatorios
</a></div><div class="nav-item"><a href="/machineLearning/veiculos/intro.html" class="nav-link">
  Veículos Autônomos
</a></div><div class="nav-item"><a href="/machineLearning/tcc/index.html" class="nav-link">
  TCC
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Machine Learning</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Deep Learning</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Veículos autônomos</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/machineLearning/relatorios/va.html" class="sidebar-link">Iniciativas relacionadas ao desenvolvimento de veículos autônomos no Brasil</a></li><li><a href="/machineLearning/relatorios/iara.html" aria-current="page" class="active sidebar-link">Iara - Prof. Alberto Ferreira de Souza</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/machineLearning/relatorios/iara.html#categorizacao-de-artigos-sumario" class="sidebar-link">Categorização de artigos &amp; sumário</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h2 id="categorizacao-de-artigos-sumario"><a href="#categorizacao-de-artigos-sumario" class="header-anchor">#</a> Categorização de artigos &amp; sumário</h2> <p>Essa seção tem por objetivo categorizar alguns dos artigos do Prof. Alberto Ferreira de Souza com ênfase em veículos autônomos.</p> <table><thead><tr><th>Índice</th> <th>Ano</th> <th>Categoria(s)</th> <th>Técnicas/Algoritmos</th></tr></thead> <tbody><tr><td><a href="#id_1">[1]</a></td> <td>Dez/2020</td> <td>Detecção de objetos (semáforos)</td> <td>Faster R-CNN (também cita YOLO)</td></tr> <tr><td><a href="#id_2">[2]</a></td> <td>Nov/2020</td> <td>Detecção de objetos (pistas)</td> <td>LaneATT (modelo proposto)</td></tr> <tr><td><a href="#id_3">[3]</a></td> <td>Jul/2020</td> <td>Detecção de objetos (sinais de trânsito)</td> <td>Faster R-CNN para a validação das imagens geradas</td></tr> <tr><td><a href="#id_4">[4]</a></td> <td>Jul/2020</td> <td>Planejamento de viagens/ caminho</td> <td>CNN nomeado DeepPath (composto de três seções: WideResNet38, DeepLabV3 e uma seção final que realiza um up-sampling bilinear)</td></tr> <tr><td><a href="#id_5">[5]</a></td> <td>Jul/2020</td> <td>Localização</td> <td>DNN nomeado NeuralMapper</td></tr> <tr><td><a href="#id_6">[6]</a></td> <td>Jul/2020</td> <td>Detecção de pistas</td> <td>CNN nomeado PolyLaneNet (com um backbone EfficientNet-b0), mas também cita SCNN, Line-CNN, ENet-SAD e FastDraw</td></tr> <tr><td><a href="#id_7">[7]</a></td> <td>Abr/2020</td> <td>Localização</td> <td>Distribuição Gaussiana, Normalized Mutual Information (NMI)</td></tr> <tr><td><a href="#id_8">[8]</a></td> <td>Nov/2019</td> <td>Detecção de objetos (pedestres) &amp; planejamento de caminhos</td> <td>YOLO v3, R-FCNN</td></tr> <tr><td><a href="#id_9">[9]</a></td> <td>Out/2019</td> <td>Predição de volume</td> <td>Generalized ICP</td></tr> <tr><td><a href="#id_10">[10]</a></td> <td>Out/2019</td> <td>Detecção de objetos (semáforos) &amp; predição de estado</td> <td>Deep Convolutional neural network (CNN) com um backbone ResNet-50 modificado</td></tr> <tr><td><a href="#id_11">[11]</a></td> <td>Jul/2019</td> <td>Geração de grades de ocupação (sem objetos não-estáticos)</td> <td>DeepLabv3+ (que usa uma variante do ResNet-101) para a Segmentação de imagem semântica e <em>Density-Based Spatial Clustering of Applications with Noise</em> (DBSCAN) para o agrupamento de pontos</td></tr> <tr><td><a href="#id_12">[12]</a></td> <td>Jul/2019</td> <td>Detecção de objetos (veículos à longas distâncias)</td> <td>YOLOv2, mas também cita ClusterNet</td></tr> <tr><td><a href="#id_13">[13]</a></td> <td>Jul/2019</td> <td>Detecção de objetos &amp; geração de datasets</td> <td>CycleGAN para geração do dataset, Faster R-CNN para treinamento e POC (também cita YOLO e RetinaNet para trabalhos futuros)</td></tr> <tr><td><a href="#id_14">[14]</a></td> <td>Jul/2019</td> <td>Detecção de objetos (sinais de trânsito) &amp; geração de datasets</td> <td>Faster R-CNN</td></tr> <tr><td><a href="#id_15">[15]</a></td> <td>Jul/2019</td> <td>Detecção de objetos (semáforo) &amp; detecção de estado dos mesmos</td> <td>YOLOv3, mas também cita Faster R-CNN e DeepTLR</td></tr> <tr><td><a href="#id_16">[16]</a></td> <td>Out/2018</td> <td>Remoção de ruído de mapas de grade de ocupação (OGM)</td> <td>Map decay, GraphSLAM</td></tr> <tr><td><a href="#id_17">[17]</a></td> <td>Ago/2018</td> <td>Estimação de direção de rumo (para alinhar o veículo à estrada)</td> <td>CNN (inspirada na AlexNet com algumas modificações)</td></tr> <tr><td><a href="#id_18">[18]</a></td> <td>Ago/2018</td> <td>Localização a partir do reconhecimento de lugares</td> <td>Rede Híbrida entre <em>Weightless Neural Networks</em> (WNN, nomeada VibGL) &amp; <em>Convolutional Neural Network</em> (CNN, baseada na VGG-16)</td></tr></tbody></table> <h3 id="template"><a href="#template" class="header-anchor">#</a> Template:</h3> <p>Para isso, o seguinte template será seguido:</p> <p>Título</p> <ul><li>Ano</li> <li>Categoria(s)</li> <li>Técnicas/ <br> Algoritmos</li> <li>Breve resumo</li></ul> <div id="id_1"></div> <h3 id="_1-deep-traffic-light-detection-by-overlaying-synthetic-context-on-arbitrary-natural-images"><a href="#_1-deep-traffic-light-detection-by-overlaying-synthetic-context-on-arbitrary-natural-images" class="header-anchor">#</a> 1 - Deep traffic light detection by overlaying synthetic context on arbitrary natural images</h3> <ul><li><strong>Ano</strong>:  Dez/2020</li> <li><strong>Categoria(s)</strong>: Detecção de objetos (semáforos)</li> <li><strong>Técnicas/Algoritmos</strong>:  Faster R-CNN (também cita YOLO)</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe a utilização de imagens geradas artificialmente para o treinamento de redes neurais de forma a sanar algumas das principais dificuldades encontradas em trabalhos anteriores.</p></blockquote> <p>Trabalhos anteriores lidavam com datasets não balenceados (ausencia de status amarelo, por exemplo) e dificuldade de se categorizar grandes quantidades de imagens</p> <p>A solução proposta se inicia pelo dataset, buscando ser o máximo balanceado possível, gerando imagens artificiais a partir de um fundo (uma imagem fora do contexto de trânsito) e um 'fronte' (uma imagem gerada artificialmente simulando um cenário de trânsito com sinais). Para validação foram usados 4 datasets: LISA (Laboratory for Intelligent &amp; Safe Automobiles), Udacity, LaRA (La Route Automatisée), e um proprietário.</p> <p>Os modelos foram treinando usando uma implementação pública disponibilizada no Tensorflow, que utiliza <strong>Faster R-CNN</strong>.</p> <div id="id_2"></div> <h3 id="_2-keep-your-eyes-on-the-lane-real-time-attention-guided-lane-detection"><a href="#_2-keep-your-eyes-on-the-lane-real-time-attention-guided-lane-detection" class="header-anchor">#</a> 2 - Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection</h3> <ul><li><strong>Ano</strong>:  Nov/2020</li> <li><strong>Categoria(s)</strong>: Detecção de objetos (pistas)</li> <li><strong>Técnicas/Algoritmos</strong>:  LaneATT (modelo proposto)</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe um modelo para a detecção de pistas chamado LaneATT, que utiliza backbones leves CNN ResNet</p></blockquote> <p>O modelo proposto, LaneATT, é um modelo de estágio único baseado em âncoras, como YOLOv3 ou SSD (Single Shot MultiBox Detector) para detecção de pistas. A detecção de âncoras é feita utilizando linhas ao invés de caixas.</p> <div id="id_3"></div> <h3 id="_3-deep-traffic-sign-detection-and-recognition-without-target-domain-real-images"><a href="#_3-deep-traffic-sign-detection-and-recognition-without-target-domain-real-images" class="header-anchor">#</a> 3 - Deep Traffic Sign Detection and Recognition Without Target Domain Real Images</h3> <ul><li><strong>Ano</strong>:  Jul/2020</li> <li><strong>Categoria(s)</strong>: Detecção de objetos (sinais de trânsito)</li> <li><strong>Técnicas/Algoritmos</strong>:  Faster R-CNN para a validação das imagens geradas</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe um novo método de geração de banco de dados que requer apenas (i) imagens naturais arbitrárias, ou seja, não requer nenhuma imagem real do domínio-alvo e (ii) modelos dos sinais de trânsito.</p></blockquote> <p>Trabalhos anteriores já propuseram tal método, porém focados apenas na localização de sinais de trânsito, e não reconhecimento dos mesmos. Para o trabalho proposto, o dataset de treinamento é gerado em três passos:</p> <div class="language- extra-class"><pre><code>- Primeiramente os templates de sinais de trânsito de interesse são obtidos;
- Depois imagens de fundo (background) que não pertencem ao domínio de interesse são obtidas (ImageNet, Microsoft COCO);
- Por fim, as amostras de treinamento são geradas, compostas por imagens com sinais de trânsito anotadas. 
</code></pre></div><p>Dessa forma, os principais problemas relacionados à dificuldade de obtenção de imagens (datasets) de sinais de trânsito são sanados: a dificuldade em se anotar datasets volumosos e também de se obtê-los.</p> <div id="id_4"></div> <h3 id="_4-image-based-real-time-path-generation-using-deep-neural-networks"><a href="#_4-image-based-real-time-path-generation-using-deep-neural-networks" class="header-anchor">#</a> 4 - Image-Based Real-Time Path Generation Using Deep Neural Networks</h3> <ul><li><strong>Ano</strong>:  Jul/2020</li> <li><strong>Categoria(s)</strong>: Planejamento de viagens/ caminho</li> <li><strong>Técnicas/Algoritmos</strong>: CNN nomeado DeepPath (composto de três seções: WideResNet38, DeepLabV3 e uma seção final que realiza um up-sampling bilinear)</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe um planejador de viagens/caminhos em tempo real baseado em imagem para o carro autônomo IARA, denominado DeepPath, que usa CNN para inferir caminhos de imagens</p></blockquote> <p>Para a elaboração de caminhos pelo DeepPath, são necessárias apenas imagens do posto de vista frontal de um veículo, enquanto outros métodos requerem imagens aéreas. Em comparação com trabalhos anteriores, o DeepPath realiza apenas o planejamento do caminho, e não do movimento. Isso se dá pelo fato de ele ser desenvolvido para o veículo autônomo IARA, cujo sistema autônomo lida com planejamento de caminho e de movimento em diferente subsistemas separados.</p> <p>Durante a operação de direção autônoma o DeepPath recebe um input (imagem) da câmera frontal do IARA e a localização atual do subsistema localizador, gerando uma saída (caminho). Tal tarefa é realizada a partir da utilização de uma CNN para inferir o modelo do caminho. O modelo do caminho é então transformado em um caminho real:</p> <div class="language- extra-class"><pre><code>- (i) gerando o caminho no sistema de coordenadas do IARA e então 
- (ii) transformando cada posição do caminho no sistema de coordenadas do IARA em outra posição no sistema de coordenadas mundial.
</code></pre></div><div id="id_5"></div> <h3 id="_5-a-large-scale-mapping-method-based-on-deep-neural-networks-applied-to-self-driving-car-localization"><a href="#_5-a-large-scale-mapping-method-based-on-deep-neural-networks-applied-to-self-driving-car-localization" class="header-anchor">#</a> 5 - A Large-Scale Mapping Method Based on Deep Neural Networks Applied to Self-Driving Car Localization</h3> <ul><li><strong>Ano</strong>:  Jul/2020</li> <li><strong>Categoria(s)</strong>: Localização</li> <li><strong>Técnicas/Algoritmos</strong>: DNN nomeado NeuralMapper</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe a criação do NeuralMapper: um subsistema da arquitetura de software de autonomia da IARA que recebe dados do sensor LiDAR como entrada e gera como saída um mapa de grade de ocupação (Occupancy Grid Mapping, OGM) ao redor do carro.</p></blockquote> <p>A arquitetura DNN do NeuralMapper consiste em três camadas: um encoder, um context module  e um decoder. Ao final de uma predição será dito se uma grade de ocupação (0.2m X 0.2m) está livre, ocupada ou desconhecida.</p> <p>A principal motivação do desenvolvimento do NeuralMapper foi a substituição do algoritmo OGM, para potencialmente reduzir a quantidade de linhas de código e a capacidade das redes neurais de lidarem com a não-linearidade dos dados. Ao final conclue-se que o NeuralMapper pode substituir o algoritmo OGM no IARA.</p> <div id="id_6"></div> <h3 id="_6-polylanenet-lane-estimation-via-deep-polynomial-regression"><a href="#_6-polylanenet-lane-estimation-via-deep-polynomial-regression" class="header-anchor">#</a> 6 - PolyLaneNet: Lane Estimation via Deep Polynomial Regression</h3> <ul><li><strong>Ano</strong>:  Jul/2020</li> <li><strong>Categoria(s)</strong>: Detecção de pistas</li> <li><strong>Técnicas/Algoritmos</strong>: CNN nomeado PolyLaneNet (com um backbone EfficientNet-b0), mas também cita SCNN, Line-CNN, ENet-SAD e FastDraw</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe a criação do PolyLaneNet, uma CNN para estimativa de marcações de faixa de ponta a ponta.</p></blockquote> <p>No trabalho proposto, a PolyLaneNet recebe de input (entrada) imagens de uma câmera frontal montada no veículo e gera polinômios que representam cada marcação de faixa na imagem, junto com os domínios para esses polinômios e pontuações de confiança para cada faixa. Trabalhos anteriores também focavam em detecção de pistas baseadas em modelos e/ou aprendizagem  de máquina ( sem redes neurais ) porém não eram tão robustos para fatores como alteração de iluminação, condições climáticas dentre outros. Alguns modelos que utilizam redes neurais foram elaborados, porém ou não possuem o código fonte liberado para o público, ou não podem ter seus resultados reproduzidos.</p> <p>Dessa forma, a PolyLaneNet se diferencia não somente por ter uma performance similar à modelos considerados <em>estado-da-arte</em>, mas também por permitir a reprodutibilidade.</p> <p>Ao final verifica-se uma precisão consideravelmente alta para linhas mais próximas à camera, e menor conforme a distância até o horizonte aumenta (por um provável desbalanceamento nos dados) e um viés da rede para linhas em detrimento de curvas acentuadas.</p> <div id="id_7"></div> <h3 id="_7-evaluating-the-limits-of-a-lidar-for-an-autonomous-driving-localization"><a href="#_7-evaluating-the-limits-of-a-lidar-for-an-autonomous-driving-localization" class="header-anchor">#</a> 7 - Evaluating the Limits of a LiDAR for an Autonomous Driving Localization</h3> <ul><li><strong>Ano</strong>:  Abr/2020</li> <li><strong>Categoria(s)</strong>: Localização</li> <li><strong>Técnicas/Algoritmos</strong>: Distribuição Gaussiana, Normalized Mutual Information (NMI)</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho apresenta uma análise para mostrar o quão simples pode ser um sensor LiDAR sem reduzir a precisão da localização que usa mapas rodoviários e de satélite juntos para posicionar globalmente o carro, e propõe uma nova técnica para localização global de um veículo.</p></blockquote> <p>No trabalho proposto, os resultados mostram que mesmo com simplificações significativas no sensor LiDAR, ainda é possível realizar a localização do veículo com precisão suficiente para algumas aplicações (com testes realizados no IARA). As simplificações se dão pela redução dos scans horizontais, linhas verticais, <em>framerate</em> e barulho (<em>noise</em>). Verifica-se que uma redução dos scans horizontais de 100% para 12% aumenta o Erro médio absoluto (<em>MAE</em>) de 0.8m para apenas 0.84m.</p> <p>Outros cenários foram testados, mostrando que uma redução nos custos e tipo de sensores LiDAR podem ser feitas sem impactar significativamente a performance dos sistemas de localização.</p> <div id="id_9"></div> <h3 id="_9-simple-and-effective-load-volume-estimation-in-moving-trucks-using-lidar"><a href="#_9-simple-and-effective-load-volume-estimation-in-moving-trucks-using-lidar" class="header-anchor">#</a> 9 - Simple and Effective Load Volume Estimation in Moving Trucks using LiDAR</h3> <ul><li><strong>Ano</strong>:  Out/2019</li> <li><strong>Categoria(s)</strong>: Predição de volume</li> <li><strong>Técnicas/Algoritmos</strong>: Generalized ICP</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe uma nova técnica para estimar o volume de cargas de caminhões em movimento usando dois sensores LiDARs.</p></blockquote> <div id="id_10"></div> <h3 id="_10-relevant-traffic-light-localization-via-deep-regression"><a href="#_10-relevant-traffic-light-localization-via-deep-regression" class="header-anchor">#</a> 10 - Relevant Traffic Light Localization via Deep Regression</h3> <ul><li><strong>Ano</strong>:  Out/2019</li> <li><strong>Categoria(s)</strong>: Detecção de objetos (semáforos) &amp; predição de estado</li> <li><strong>Técnicas/Algoritmos</strong>: Deep Convolutional neural network (CNN) com um backbone ResNet-50 modificado</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe uma nova técnica para estimar o volume de cargas de caminhões em movimento usando dois sensores LiDARs.</p></blockquote> <p>Um dos problemas enfrentados por veículos autônomos é a detecção de estado de um semáforo (vermelho, amarelo ou verde). Tal problema ja foi abordado por alguns autores, porém envolve a utilização de uma série de sensores consideravelmente caros (LiDAR, câmeras, detectores de luz, dentre outros) e funciona em cenários bem restritos.</p> <p>Para resolver esse problema, o trabalho propõe um modelo que utiliza um modelo de regressão profunda (<em>deep regression model</em>) para prever as coordenadas bidimensionais de um semáforo numa imagem, obtendo uma acurácia indo de 80.22 até 88.59%, a depender do cenário de avaliação.</p> <div id="id_11"></div> <h3 id="_11-removing-movable-objects-from-grid-maps-of-self-driving-cars-using-deep-neural-networks"><a href="#_11-removing-movable-objects-from-grid-maps-of-self-driving-cars-using-deep-neural-networks" class="header-anchor">#</a> 11 - Removing Movable Objects from Grid Maps of Self-Driving Cars Using Deep Neural Networks</h3> <ul><li><strong>Ano</strong>:  Jul/2019</li> <li><strong>Categoria(s)</strong>: Geração de grades de ocupação (sem objetos não-estáticos)</li> <li><strong>Técnicas/Algoritmos</strong>:  DeepLabv3+ (que usa uma variante do ResNet-101) para a Segmentação de imagem semântica e <em>Density-Based Spatial Clustering of Applications with Noise</em> (DBSCAN) para o agrupamento de pontos</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe uma técnica para remover vestígios de objetos móveis de mapas de grade de ocupação (OGM) com base em redes neurais profundas.</p></blockquote> <p>Até uma data anterior ao momento de publicação do paper, um dos problemas que a IARA enfrentava era não possuir uma forma de gerar mapas de grade de ocupação (OGM) de forma automática, o que forçava a necessidade de um trabalho manual para a geração dos mesmos.</p> <p>Dessa forma, o trabalho propõe uma técnica chamada <em>Enhanced OGM generation</em> (E-OGM-G), baseada na técnica <em>Large-Scale Environment Mapping System</em> (LEMS), para gerar OGM &quot;limpos&quot;. A técnica recebe como entrada imagens e nuvens de pontos, sendo processada com uma Segmentação de imagem semântica, remoção de piso, agrupamento de pontos e remoção de objetos móveis para, por fim, gerar um OGM limpo.</p> <div id="id_12"></div> <h3 id="_12-bio-inspired-foveated-technique-for-augmented-range-vehicle-detection-using-deep-neural-networks"><a href="#_12-bio-inspired-foveated-technique-for-augmented-range-vehicle-detection-using-deep-neural-networks" class="header-anchor">#</a> 12 - Bio-Inspired Foveated Technique for Augmented-Range Vehicle Detection Using Deep Neural Networks</h3> <ul><li><strong>Ano</strong>:  Jul/2019</li> <li><strong>Categoria(s)</strong>: Detecção de objetos (veículos à longas distâncias)</li> <li><strong>Técnicas/Algoritmos</strong>: YOLOv2, mas também cita ClusterNet</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe uma técnica de renderização otimizada (<em>foveat</em>) bio-inspirada para detectar carros em uma câmera de visão de longo alcance usando uma rede neural convolucional profunda</p></blockquote> <p>A técnica proposta no trabalho parte da ideia de que os humanos conseguem detectar facilmente outros veículos a longas distâncias observando determinados pontos no horizonte; a partir disso propõe-se a elaboração de um modelo que utiliza como entrada imagens de câmera (ao contrário de dados de sensores RADAR ou LiDAR, que são consideravelmente mais caros).</p> <p>O <em>augmented-range vehicle detection system</em>(ARVDS, modelo proposto) funciona da seguinte forma: uma imagem de entrada é recebida pelo sistema a partir da câmera. A partir do sistema de projeção de <em>waypoints</em> do IARA são realizados cortes na imagem. A partir da imagem original e dos cortes gerados é aplicada a rede YOLOv2 para a detecção de veículos.</p> <p>Ao final , o ARVDS foi capaz de aumentar a precisão de detecção de veículos distantes de 29.51% para 63.15%.</p> <div id="id_13"></div> <h3 id="_13-cross-domain-car-detection-using-unsupervised-image-to-image-translation-from-day-to-night"><a href="#_13-cross-domain-car-detection-using-unsupervised-image-to-image-translation-from-day-to-night" class="header-anchor">#</a> 13 - Cross-Domain Car Detection Using Unsupervised Image-to-Image Translation: From Day to Night</h3> <ul><li><strong>Ano</strong>:  Jul/2019</li> <li><strong>Categoria(s)</strong>: Detecção de objetos &amp; geração de datasets</li> <li><strong>Técnicas/Algoritmos</strong>: CycleGAN para geração do dataset, Faster R-CNN para treinamento e POC (também cita YOLO e RetinaNet para trabalhos futuros)</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe um método para treinar um sistema de detecção de um veículo autônomo com dados anotados de um domínio de origem (imagens diurnas) sem exigir as anotações de imagem do domínio alvo (imagens noturnas)</p></blockquote> <p>O trabalho cita a dificuldade de treinar e testar modelos de alta-performance em determinado domínio, mas o modelo é treinado em  um domínio distinto, porém análogo (por exemplo, ter a intenção de se detectar semáforos em cenas noturnas mas só possuir um dataset com imagens de semáforos durante o dia).</p> <p>Para isso, o modelo proposto recebe como input imagens diurnas anotadas (com bounding boxes) e imagens noturnas sem nenhum tipo de anotação, e gera imagens falsas utilizando a rede CycleGAN. As anotações são então copiadas para as imagens falsas, que agora possuem o contexto noturno sem alterar fatos importantes (como posições de veículos, sinais de trânsito e semáforos). Na sequência, é escolhido um framework Faster R-CNN para a detecção de objetos.</p> <p>Ao final, verifica-se que a adição das imagens geradas artificialmente é capaz de melhorar a performance do modelo de detecção de objetos em 10.7%. Também é sugerido como trabalhos futuros utilizar outros detectores de objetos &quot;estado-da-arte&quot; como YOLO e RetinaNet</p> <div id="id_14"></div> <h3 id="_14-effortless-deep-training-for-traffic-sign-detection-using-templates-and-arbitrary-natural-images"><a href="#_14-effortless-deep-training-for-traffic-sign-detection-using-templates-and-arbitrary-natural-images" class="header-anchor">#</a> 14 - Effortless Deep Training for Traffic Sign Detection Using Templates and Arbitrary Natural Images</h3> <ul><li><strong>Ano</strong>:  Jul/2019</li> <li><strong>Categoria(s)</strong>: Detecção de objetos (sinais de trânsito) &amp; geração de datasets</li> <li><strong>Técnicas/Algoritmos</strong>: Faster R-CNN</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe um método de geração de datasets que requer apenas: (i) imagens naturais arbitrárias, ou seja, não requer nenhuma imagem real do domínio de interesse, e (ii) modelos dos sinais de trânsito, ou seja, modelos criados sinteticamente para ilustrar a aparência da categoria de um sinal de trânsito</p></blockquote> <p>Trabalhos anteriores já propuseram tal método, porém focados apenas na localização de sinais de trânsito, e não reconhecimento dos mesmos. Para o trabalho proposto, o dataset de treinamento é gerado em três passos:</p> <ul><li>Primeiramente os templates de sinais de trânsito de interesse são obtidos;</li> <li>Depois imagens de fundo (background) que não pertencem ao domínio de interesse são obtidas (ImageNet, Microsoft COCO);</li> <li>Por fim, as amostras de treinamento são geradas, compostas por imagens com sinais de trânsito anotadas.</li></ul> <p>Dessa forma, os principais problemas relacionados à dificuldade de obtenção de imagens (datasets) de sinais de trânsito são sanados: a dificuldade em se anotar datasets volumosos e também de se obtê-los.</p> <p>Obs.: O trabalho é muito parecido ao realizado em <a href="#id_3">[3]</a>. O principal ponto de diferença entre eles é:</p> <ul><li>Enquanto <a href="#id_3">[3]</a> foca em descobrir cada classe dos sinais de trânsito (ex.: diferenciar placas de velocidade com valores diferentes), <a href="#id_14">[14]</a> foca apenas na detecção do mesmo, sem classificá-los;</li></ul> <div id="id_15"></div> <h3 id="_15-traffic-light-recognition-using-deep-learning-and-prior-maps-for-autonomous-cars"><a href="#_15-traffic-light-recognition-using-deep-learning-and-prior-maps-for-autonomous-cars" class="header-anchor">#</a> 15 - Traffic Light Recognition Using Deep Learning and Prior Maps for Autonomous Cars</h3> <ul><li><strong>Ano</strong>:  Jul/2019</li> <li><strong>Categoria(s)</strong>: Detecção de objetos (semáforo) &amp; detecção de estado dos mesmos</li> <li><strong>Técnicas/Algoritmos</strong>: YOLOv3, mas também cita Faster R-CNN e DeepTLR</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe integrar o poder de detecção baseada em aprendizagem profunda com os mapas anteriores usados pela <em>IARA</em> para reconhecer os semáforos relevantes de rotas predefinidas.</p></blockquote> <p>Trabalhos anteriores já abordaram a questão de reconhecimento de semáforos, porém nenhum se propôs a utilizar redes neurais em conjunto com dados prévios de mapas para &quot;avisar&quot; onde potencialmente haverá um sinal.</p> <p>Dessa forma, o trabalho propõe utilizar os dados de mapas gerados pela IARA através dos sensores LiDAR e câmeras, em conjunto com a rede YOLOv3, para detectar semáforos e classificar o estado dos mesmos.</p> <p>O treinamento e validação do modelo foi realizado utilizando os datasets <em>DriveU Traffic Light Dataset</em> (DTLD) e <em>LISA Traffic Light Dataset</em> (LISATLD), ambos disponíveis de forma pública. Ao final a precisão variou de 60.86 à 88.59%, a depender do cenário e variáveis avaliadas.</p> <div id="id_16"></div> <h3 id="_16-memory-like-map-decay-for-autonomous-vehicles-based-on-grid-maps"><a href="#_16-memory-like-map-decay-for-autonomous-vehicles-based-on-grid-maps" class="header-anchor">#</a> 16 - Memory-like Map Decay for Autonomous Vehicles based on Grid Maps</h3> <ul><li><strong>Ano</strong>:  Out/2018</li> <li><strong>Categoria(s)</strong>: Remoção de ruído de mapas de grade de ocupação (OGM)</li> <li><strong>Técnicas/Algoritmos</strong>: Map decay, GraphSLAM</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe uma estratégia para corrigir imperfeições em mapas de grade de ocupação (OGM) denominada decaimento de mapa (Map Decay),</p></blockquote> <p>O algoritmo proposto no trabalho, <em>Map decay</em>, usa como inspiração as habilidades do cérebro de liberar informações que não são mais necessárias da memória de curto prazo e de dar sentido a dados sensoriais incompletos, preenchendo-os com conhecimento de longo prazo. Dessa forma, ele torna-se uma solução simples e eficiente para lidar com imperfeições causadas por objetos em movimento.</p> <div id="id_17"></div> <h3 id="_17-heading-direction-estimation-using-deep-learning-with-automatic-large-scale-data-acquisition"><a href="#_17-heading-direction-estimation-using-deep-learning-with-automatic-large-scale-data-acquisition" class="header-anchor">#</a> 17 - Heading Direction Estimation Using Deep Learning with Automatic Large-scale Data Acquisition</h3> <ul><li><strong>Ano</strong>:  Ago/2018</li> <li><strong>Categoria(s)</strong>: Estimação de direção de rumo (para alinhar o veículo à estrada)</li> <li><strong>Técnicas/Algoritmos</strong>: CNN (inspirada na AlexNet com algumas modificações)</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe uma abordagem para o problema de estimar a direção de rumo que mantém o veículo alinhado com a direção da estrada para Sistemas avançados de assistência ao motorista (do inglês <em>ADAS</em>)</p></blockquote> <p>O sistema proposto utiliza plataformas públicas disponíveis (como <em>Open Street Maps</em> e <em>Google street view</em>) para a obtenção dos dados e anotação dos mesmos, obtendo uma base de dados consideravelmente grande. Na sequência, a base de dados obtida é utilizada para treinar uma rede CNN que irá atacar o problema proposto, recebendo como entrada uma única imagem, prevendo a diferença na orientação atual do carro e a orientação &quot;ideal&quot; do carro 4 metros à frente na estrada.</p> <p>Trabalhos anteriores já buscaram abordar esse problema, porém caiam em questões de complexidade algorítmica, dificuldade de generalização ou utilização de sensores caros, como LiDAR.</p> <div id="id_18"></div> <h3 id="_18-visual-global-localization-with-a-hybrid-wnn-cnn-approach"><a href="#_18-visual-global-localization-with-a-hybrid-wnn-cnn-approach" class="header-anchor">#</a> 18 - Visual Global Localization with a Hybrid WNN-CNN Approach</h3> <ul><li><strong>Ano</strong>:  Ago/2018</li> <li><strong>Categoria(s)</strong>: Localização a partir do reconhecimento de lugares</li> <li><strong>Técnicas/Algoritmos</strong>: Rede Híbrida entre <em>Weightless Neural Networks</em> (WNN, nomeada VibGL) &amp; <em>Convolutional Neural Network</em> (CNN, baseada na VGG-16)</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe uma abordagem híbrida utilizando redes WNN e CNN para resolver o problema de localização global usando as informações topológicas e métricas para aproximar a posição atual do veículo, sendo bastante útil em ambientes onde não é possível utilizar GPS ()</p></blockquote> <p>O paper ja abordou o problema de reconhecimento de lugar (<em>place recognition</em>) em trabalhos anteriores, gerando um sistema de reconhecimendo conhecido como <em>VibGL</em> (WNN).</p> <p>Para o trabalho proposto, a abordagem é dupla: (i) um WNN para resolver o reconhecimento de lugar como um problema de classificação e (ii) uma CNN para resolver a localização visual (<em>visual localization</em>) como um problema de regressão métrica.</p> <p>O sistema funciona da seguinte forma: dada uma imagem de câmera ao vivo, o primeiro sistema (i) recupera a imagem mais semelhante e sua pose associada, enquanto o último (ii) compara a imagem coletada com a imagem de câmera ao vivo para gerar uma pose relativa 6D. O resultado final é uma estimativa da pose atual do robô/veículo autônomo (IARA)</p> <p>Ao final, o sistema é capaz de localizar o veículo 90% do tempo com um erro médio de 1.2m, contra 1.12m do sistema <em>SLAM</em> e 0.37m do GPS, ambos com 89% de localização.</p> <div id="id_19"></div> <h3 id="_19-mapping-road-lanes-using-laser-remission-and-deep-neural-networks"><a href="#_19-mapping-road-lanes-using-laser-remission-and-deep-neural-networks" class="header-anchor">#</a> 19 - Mapping road lanes using laser remission and deep neural networks</h3> <ul><li><strong>Ano</strong>:  Ago/2018</li> <li><strong>Categoria(s)</strong>:</li> <li><strong>Técnicas/Algoritmos</strong>: Efficient Neural Network (ENet) DNN</li> <li><strong>Breve resumo</strong>:</li></ul> <blockquote><p>O trabalho propõe a utilização de redes neurais profundas (DNN) para resolver o problema de inferir a posição e propriedades relevantes de vias urbanas com sinalização horizontal (faixas/marcações na pista) pobre ou ausente, de forma a permitir a operação de carros autônomos em tais situações.</p></blockquote> <p>O trabalho cita o fato de que, anteriormente ao projeto desenvolvido, o IARA (veículo autônomo), não possuia um mecanismo automático para inferir as faixas das estradas, utilizando <em>Road Definition Data Files</em> (RDDF).</p> <p>Para o desenvolvimento da rede, um dataset foi elaborado a partir da construção de mapas de estradas utilizando dados do sensor LiDAR (Laser Remission) na região da UFES. O dataset foi anotado manualmente, e utilizado para o treinamento e validação da rede DNN.</p> <p>Ao final do experimento, obteve-se uma acurácia média de 83.7%. Tal valor, de acordo com o paper, poderia ser superior, visto que a anotação manual possui imperfeições por ser bastante trabalhosa, impactando no treinamento da rede.</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/machineLearning/relatorios/va.html" class="prev">
        Iniciativas relacionadas ao desenvolvimento de veículos autônomos no Brasil
      </a></span> <!----></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/machineLearning/assets/js/app.85d7bd56.js" defer></script><script src="/machineLearning/assets/js/2.4f8302c5.js" defer></script><script src="/machineLearning/assets/js/25.40c583f5.js" defer></script>
  </body>
</html>
