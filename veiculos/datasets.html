<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Datasets | Estudo e Aplicação de Técnicas de Machine Learning</title>
    <meta name="generator" content="VuePress 1.9.7">
    
    <meta name="description" content="Todos os relatórios e afins sobre machine learning, deep learning e Visão computacional estarão aqui.">
    
    <link rel="preload" href="/machineLearning/assets/css/0.styles.f80be0c7.css" as="style"><link rel="preload" href="/machineLearning/assets/js/app.85d7bd56.js" as="script"><link rel="preload" href="/machineLearning/assets/js/2.4f8302c5.js" as="script"><link rel="preload" href="/machineLearning/assets/js/30.82d06992.js" as="script"><link rel="prefetch" href="/machineLearning/assets/js/10.29b60263.js"><link rel="prefetch" href="/machineLearning/assets/js/11.c63c2c0c.js"><link rel="prefetch" href="/machineLearning/assets/js/12.78249c11.js"><link rel="prefetch" href="/machineLearning/assets/js/13.fcdeb0ea.js"><link rel="prefetch" href="/machineLearning/assets/js/14.ced299f6.js"><link rel="prefetch" href="/machineLearning/assets/js/15.df6f2744.js"><link rel="prefetch" href="/machineLearning/assets/js/16.58f755ed.js"><link rel="prefetch" href="/machineLearning/assets/js/17.ce969e99.js"><link rel="prefetch" href="/machineLearning/assets/js/18.47c54fbb.js"><link rel="prefetch" href="/machineLearning/assets/js/19.73db5433.js"><link rel="prefetch" href="/machineLearning/assets/js/20.4168cf0b.js"><link rel="prefetch" href="/machineLearning/assets/js/21.320089d5.js"><link rel="prefetch" href="/machineLearning/assets/js/22.1740eda5.js"><link rel="prefetch" href="/machineLearning/assets/js/23.269b9116.js"><link rel="prefetch" href="/machineLearning/assets/js/24.850bbadc.js"><link rel="prefetch" href="/machineLearning/assets/js/25.40c583f5.js"><link rel="prefetch" href="/machineLearning/assets/js/26.d2dc0497.js"><link rel="prefetch" href="/machineLearning/assets/js/27.c3a0af4f.js"><link rel="prefetch" href="/machineLearning/assets/js/28.a01a0b69.js"><link rel="prefetch" href="/machineLearning/assets/js/29.c2a025ff.js"><link rel="prefetch" href="/machineLearning/assets/js/3.7ee3d1e9.js"><link rel="prefetch" href="/machineLearning/assets/js/31.855a9550.js"><link rel="prefetch" href="/machineLearning/assets/js/4.c3fb2158.js"><link rel="prefetch" href="/machineLearning/assets/js/5.02305c60.js"><link rel="prefetch" href="/machineLearning/assets/js/6.4a48f0f1.js"><link rel="prefetch" href="/machineLearning/assets/js/7.7322ff41.js"><link rel="prefetch" href="/machineLearning/assets/js/8.198f804d.js"><link rel="prefetch" href="/machineLearning/assets/js/9.0acdc198.js">
    <link rel="stylesheet" href="/machineLearning/assets/css/0.styles.f80be0c7.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/machineLearning/" class="home-link router-link-active"><!----> <span class="site-name">Estudo e Aplicação de Técnicas de Machine Learning</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/machineLearning/artigos/sobre.html" class="nav-link">
  Artigos
</a></div><div class="nav-item"><a href="/machineLearning/glossario/funcoes/" class="nav-link">
  Glossário
</a></div><div class="nav-item"><a href="/machineLearning/relatorios/relatorio.html" class="nav-link">
  Relatorios
</a></div><div class="nav-item"><a href="/machineLearning/veiculos/intro.html" class="nav-link">
  Veículos Autônomos
</a></div><div class="nav-item"><a href="/machineLearning/tcc/index.html" class="nav-link">
  TCC
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/machineLearning/artigos/sobre.html" class="nav-link">
  Artigos
</a></div><div class="nav-item"><a href="/machineLearning/glossario/funcoes/" class="nav-link">
  Glossário
</a></div><div class="nav-item"><a href="/machineLearning/relatorios/relatorio.html" class="nav-link">
  Relatorios
</a></div><div class="nav-item"><a href="/machineLearning/veiculos/intro.html" class="nav-link">
  Veículos Autônomos
</a></div><div class="nav-item"><a href="/machineLearning/tcc/index.html" class="nav-link">
  TCC
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Artigos &amp; Datasets</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/machineLearning/veiculos/datasets.html" aria-current="page" class="active sidebar-link">Datasets</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/machineLearning/veiculos/datasets.html#pedestres" class="sidebar-link">Pedestres</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h2 id="pedestres"><a href="#pedestres" class="header-anchor">#</a> Pedestres</h2> <p>Datasets que envolvem pedestres, obstruídos ou não.</p> <h3 id="cvc-05-partially-occluded-pedestrian-dataset"><a href="#cvc-05-partially-occluded-pedestrian-dataset" class="header-anchor">#</a> CVC-05 Partially Occluded Pedestrian Dataset</h3> <p><img src="http://adas.cvc.uab.es/elektra/wp-content/uploads/sites/13/2016/05/CVC05_Frame.png" alt=""></p> <p><strong>Fonte:</strong> <a href="http://adas.cvc.uab.es/elektra/enigma-portfolio/cvc-05-partially-occluded-pedestrian-dataset/" target="_blank" rel="noopener noreferrer">http://adas.cvc.uab.es/elektra/enigma-portfolio/cvc-05-partially-occluded-pedestrian-dataset/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><strong>Sobre:</strong> Este é um conjunto de dados de pedestres parcialmente obstruídos. O conjunto de dados consiste em 593 quadros positivos com pedestres anotados (com seus espelhos horizontais correspondentes).</p> <p><strong>Citação:</strong> J. Marín, D. Vázquez, A.M. López, J. Amores and L.I. Kuncheva, &quot;Occlusion handling via random subspace classifiers for human detection&quot;, In IEEE Transactions on Systems, Man, and Cybernetics (Part B), 2013.</p> <h3 id="daimler-pedestrian-detection-benchmark-dataset"><a href="#daimler-pedestrian-detection-benchmark-dataset" class="header-anchor">#</a> Daimler Pedestrian Detection Benchmark Dataset</h3> <p><img src="http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Multi-Cue_Occluded_Ped/multi_cue_2.jpg" alt=""></p> <p><strong>Fonte:</strong> <a href="http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Multi-Cue_Occluded_Ped/daimler_multi-cue_occluded_ped.html" target="_blank" rel="noopener noreferrer">http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Multi-Cue_Occluded_Ped/daimler_multi-cue_occluded_ped.html<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><strong>Sobre:</strong> Os exemplos de treino e testes consistem em <em>bounding boxes</em> rotuladas manualmente para pedestres e não-pedestres em imagens capturadas de um equipamento de câmera calibrado montado em um veículo em um ambiente urbano.</p> <p><strong>Citação:</strong> M. Enzweiler, A. Eigenstetter, B. Schiele and D. M. Gavrila,
Multi-Cue Pedestrian Classification with Partial Occlusion Handling,
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.</p> <h3 id="cuhk-occlusion-dataset"><a href="#cuhk-occlusion-dataset" class="header-anchor">#</a> CUHK Occlusion Dataset</h3> <p><img src="http://mmlab.ie.cuhk.edu.hk/images/datasets/cuhk_occlusion_large.jpg" alt=""></p> <p><strong>Fonte:</strong> <a href="http://mmlab.ie.cuhk.edu.hk/datasets/cuhk_occlusion/index.html" target="_blank" rel="noopener noreferrer">http://mmlab.ie.cuhk.edu.hk/datasets/cuhk_occlusion/index.html<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><strong>Sobre:</strong> O conjunto de dados de oclusão CUHK é para pesquisas sobre análise de atividade e cenas lotadas. Este conjunto de dados contém 1.063 imagens com pedestres obstruídos dos conjuntos de dados do Caltech [1], ETHZ [2], TUD-Bruxelas [3], INRIA [4], Caviar [5] e imagens coletadas pelo MMLab</p> <p><strong>Citação:</strong> A Discriminative Deep Model for Pedestrian Detection with Occlusion Handling W. Ouyang and X. Wang IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), 2012</p> <h3 id="widerperson-a-diverse-dataset-for-dense-pedestrian-detection-in-the-wild"><a href="#widerperson-a-diverse-dataset-for-dense-pedestrian-detection-in-the-wild" class="header-anchor">#</a> WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild</h3> <p><img src="http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/files/intro.jpg" alt=""></p> <p><strong>Fonte:</strong> <a href="http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/" target="_blank" rel="noopener noreferrer">http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><strong>Sobre:</strong> O conjunto de dados WiderPerson é um conjunto de dados de referência de detecção de pedestres em &quot;estado selvagem&quot; (<em>in the wild</em>), cujas imagens são selecionadas a partir de uma ampla gama de cenários, não mais limitados ao cenário de tráfego. Foram escolhidas 13.382 imagens e rotuladas cerca de 400 mil anotações com vários tipos de oclusões</p> <p><strong>Citação:</strong> S. Zhang, Y. Xie, J. Wan, H. Xia, S. Z. Li, and G. Guo, “Widerperson:A  diverse  dataset  for  dense  pedestrian  detection  in  the  wild,” IEEETransactions on Multimedia (TMM), 2019.</p> <h3 id="caltech-pedestrian-detection-benchmark"><a href="#caltech-pedestrian-detection-benchmark" class="header-anchor">#</a> Caltech Pedestrian Detection Benchmark</h3> <p><img src="http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/files/peds01_web.jpg" alt=""></p> <p><strong>Fonte:</strong> <a href="http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/" target="_blank" rel="noopener noreferrer">http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><strong>Sobre:</strong> O <em>Caltech Pedestrian Dataset</em> consiste em aproximadamente 10 horas de vídeo obtidos de um veículo em trânsito regular em um ambiente urbano. Cerca de 250.000 quadros (em 137 segmentos de aproximadamente minutos) com um total de 350.000 <em>bounding boxes</em> e 2.300 pedestres únicos foram anotados. A anotação inclui correspondência temporal entre <em>bounding boxes</em> e rótulos de oclusão detalhados.</p> <p><strong>Citação:</strong> ...</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/machineLearning/assets/js/app.85d7bd56.js" defer></script><script src="/machineLearning/assets/js/2.4f8302c5.js" defer></script><script src="/machineLearning/assets/js/30.82d06992.js" defer></script>
  </body>
</html>
