(window.webpackJsonp=window.webpackJsonp||[]).push([[22],{443:function(a,e,o){"use strict";o.r(e);var s=o(56),t=Object(s.a)({},(function(){var a=this,e=a.$createElement,o=a._self._c||e;return o("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[o("h2",{attrs:{id:"multinominal-naive-bayes"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#multinominal-naive-bayes"}},[a._v("#")]),a._v(" Multinominal Naive bayes")]),a._v(" "),o("p",[a._v("O classificador "),o("strong",[a._v("multinomial Naive Bayes")]),a._v(" é adequado para classificação com recursos discretos (por exemplo, contagem de palavras para classificação de texto). Ele utiliza o "),o("strong",[a._v("maximum a posteriori")]),a._v(" como regra de decisão. Ele estima a probabilidade condicional de uma palavra específica dada uma classe como a frequência relativa do termo t em documentos pertencentes à classe (c). A variação leva em consideração o número de ocorrências do termo t nos documentos de treinamento da classe (c), incluindo várias ocorrências.")]),a._v(" "),o("p",[o("img",{attrs:{src:"https://miro.medium.com/max/300/1*DdFz2o7Jt7uFzAGP8Pwdrg.png",alt:""}})]),a._v(" "),o("h3",{attrs:{id:"parametros-do-multinomialnb"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#parametros-do-multinomialnb"}},[a._v("#")]),a._v(" Parâmetros do MultinomialNB")]),a._v(" "),o("p",[o("strong",[a._v("alpha")]),a._v(" : float, optional (default=1.0):\nParâmetro de suavização aditivo (Laplace se == 1/ Lidstone se < 1 ou 0 para sem suavização).\nA idéia básica é aumentar as probabilidades de todos os bigramas (sequências de duas palavras) na equação de probabilidades não máxima  em "),o("strong",[a._v("n")]),a._v(" para tornar tudo diferente de zero. -- Hiperparâmetro")]),a._v(" "),o("p",[o("strong",[a._v("fit_prior")]),a._v(" : boolean, optional (default=True)\nSe deve aprender as probabilidades anteriores da classe ou não. Se falso, um uniforme anterior será usado.")]),a._v(" "),o("p",[o("strong",[a._v("class_prior")]),a._v(" : array-like, size (n_classes,), optional (default=None)\nProbabilidades anteriores das classes. Se especificado, os anteriores não são ajustados de acordo com os dados.")]),a._v(" "),o("h2",{attrs:{id:"adaboostclassifier"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#adaboostclassifier"}},[a._v("#")]),a._v(" AdaBoostClassifier")]),a._v(" "),o("p",[a._v("Segundo a documentação, "),o("em",[a._v("Um classificador AdaBoost é um meta-estimador que começa ajustando um classificador no conjunto de dados original e depois ajusta cópias adicionais do classificador no mesmo conjunto de dados, mas onde os pesos das instâncias classificadas incorretamente são ajustados para que os classificadores subsequentes se concentrem mais em casos difíceis")]),a._v(".")]),a._v(" "),o("p",[a._v("Na prática, ele combina diversos algoritmos de classificação fracos e transforma todos num unico forte. Um algoritmo de classificação fraco é um algoritmo simples que tem um desempenho ruim, mas que funciona melhor do que ficar adivinhando respostas aleatórias.")]),a._v(" "),o("p",[a._v("Ele pode ser aplicado em qualquer algoritmo de classificação de dados, pois a sua tecnica é baseada em muitos classificadores diferentes e não num unico só, talvez ele possa te retornar um melhor resultado.")]),a._v(" "),o("h3",{attrs:{id:"parametros-adaboostclassifier"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#parametros-adaboostclassifier"}},[a._v("#")]),a._v(" Parâmetros AdaBoostClassifier")]),a._v(" "),o("p",[o("strong",[a._v("base_estimator")]),a._v(" : object, optional (default=None)\nO estimador base a partir do qual o conjunto impulsionado (Boost) é construído. É necessário suporte para ponderação de amostra, além de atributos classes_ e n_classes_ adequados. Se "),o("strong",[a._v("None")]),a._v(", o estimador base é "),o("strong",[a._v("DecisionTreeClassifier")]),a._v(" (max_depth = 1)")]),a._v(" "),o("p",[o("strong",[a._v("n_estimators")]),a._v(" : integer, optional (default=50)\nO número máximo de estimadores nos quais o Boost é finalizado. Em caso de "),o("strong",[a._v("perfect fit")]),a._v(", o procedimento de aprendizado é finalizado antes.")]),a._v(" "),o("p",[o("strong",[a._v("learning_rate")]),a._v(" : float, optional (default=1.)\nA taxa de aprendizado reduz a contribuição de cada classificador por learning_rate. Há um trade-off entre "),o("strong",[a._v("learning_rate")]),a._v(" e "),o("strong",[a._v("n_estimators")]),a._v(".\nLearning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators.")]),a._v(" "),o("p",[o("strong",[a._v("algorithm")]),a._v(' : {‘SAMME’, ‘SAMME.R’}, optional (default=’SAMME.R’)\nSe "SAMME.R", então usa o algoritmo de '),o("em",[a._v("boosting")]),a._v(" real "),o("strong",[a._v("SAMME.R")]),a._v(". "),o("strong",[a._v("base_estimator")]),a._v(' deve suportar o cálculo de probabilidades de classe. Se "SAMME", use o algoritmo de '),o("em",[a._v("boosting")]),a._v(" discreto "),o("strong",[a._v("SAMME")]),a._v(". O algoritmo SAMME.R normalmente converge mais rápido que o SAMME, obtendo um erro de teste mais baixo com menos "),o("em",[a._v("boosting's")]),a._v(" de aumento.")]),a._v(" "),o("p",[o("strong",[a._v("random_state")]),a._v(" : int, RandomState instance or None, optional (default=None)\nSe um inteiro, "),o("strong",[a._v("random_state")]),a._v(" é a "),o("em",[a._v("seed")]),a._v(" usada pelo gerador de números aleatórios; Se uma instância "),o("em",[a._v("RandomState")]),a._v(", "),o("strong",[a._v("random_state")]),a._v(" se torna o gerador de números aleatórios; Se "),o("strong",[a._v("None")]),a._v(", o gerador de números aleatórios é a instância "),o("strong",[a._v("RandomState")]),a._v(" usada pelo "),o("strong",[a._v("np.random")]),a._v(".")]),a._v(" "),o("h2",{attrs:{id:"curiosidades"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#curiosidades"}},[a._v("#")]),a._v(" Curiosidades")]),a._v(" "),o("p",[o("em",[a._v("At a high level, AdaBoost is similar to Random Forest in that they both tally up the predictions made by each decision trees within the forest to decide on the final classification")]),a._v(" (https://towardsdatascience.com/machine-learning-part-17-boosting-algorithms-adaboost-in-python-d00faac6c464)")]),a._v(" "),o("h2",{attrs:{id:"referencias"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#referencias"}},[a._v("#")]),a._v(" Referencias")]),a._v(" "),o("h3",{attrs:{id:"multinomialnb"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#multinomialnb"}},[a._v("#")]),a._v(" MultinomialNB")]),a._v(" "),o("ul",[o("li",[o("p",[a._v("https://www.quora.com/How-does-multinomial-Naive-Bayes-work")])]),a._v(" "),o("li",[o("p",[a._v("https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html")])]),a._v(" "),o("li",[o("p",[a._v("https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes")])]),a._v(" "),o("li",[o("p",[a._v("https://datascience.stackexchange.com/questions/30473/how-does-the-mutlinomial-bayess-alpha-parameter-affects-the-text-classificati")])])]),a._v(" "),o("h3",{attrs:{id:"adaboostclassifier-2"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#adaboostclassifier-2"}},[a._v("#")]),a._v(" AdaBoostClassifier")]),a._v(" "),o("ul",[o("li",[o("p",[a._v("https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html")])]),a._v(" "),o("li",[o("p",[a._v("https://blog.dfrnks.com/2018/07/15/machine-learning-algoritmo-AdaBoostClassifier.html")])])])])}),[],!1,null,null,null);e.default=t.exports}}]);